# AvaliaÃ§Ã£o Quantitativa de Resumos: **OpenAIÂ GPTâ€‘4(o)-mini** vs. **Llamaâ€‘3Â 70B**

> **Fonte Ãºnica da verdade:** todo o procedimento descrito aqui existe, linha a linha, no notebook **`summaries_evaluation.ipynb`**.  Este README Ã© apenas um guia textual. Siga o notebook para obter exatamente os mesmos passos, fÃ³rmulas e resultados.

---

## ğŸ¯ Objetivo

Comparar, de forma transparente e reproduzÃ­vel, a qualidade de resumos gerados por dois LLMs distintos.  A avaliaÃ§Ã£o combina **julgamento humano** e **mÃ©trica automÃ¡tica (RAGAS)**, controlando a confiabilidade dos avaliadores por meio do **ICC(3,k)** e corrigindo o peso dado Ã  mÃ©trica automÃ¡tica em funÃ§Ã£o da **correlaÃ§Ã£o de Spearman (Ï)**.

---

## ğŸ“‚ Dados de Entrada

`data/summaries_evaluation.csv` â€” cada linha contÃ©m:

| Coluna                           | DescriÃ§Ã£o                                      |
| -------------------------------- | ---------------------------------------------- |
| `text`                           | Textoâ€‘fonte (input a ser resumido)             |
| `summary_openai` / `summary_llama` | Resumo gerado por cada modelo                |
| `h1_*_rating` â€¦ `h3_*_rating`    | Notas de trÃªs avaliadores humanos (0-1)        |
| `ragas_*_rating`                 | Score do RAGAS para o respectivo resumo       |
| `class_label`                    | <small>(removido na anÃ¡lise)</small>           |

O notebook elimina linhas/colunas com valores nulos ou irrelevantes e cria dois subconjuntos bem definidos:

* **`df_openai`** â€” texto + colunas `OPENAI_COLS`
* **`df_llama`**  â€” texto + colunas `LLAMA_COLS`

---

## ğŸ”¬ Pipeline AnalÃ­tico (passo a passo)

1. **Carregamento & limpeza** (`pandas`).
2. **Confiabilidade dos avaliadores**
   * Calculaâ€se o **ICC(3,k)** para cada modelo.
3. **Estimativa da nota humana agregada**
   * **OpenAI:** mediana â†’ `H_rob` (robusto a outliers).
   * **LLaMA:** mÃ©dia ponderada â†’ `H_bar`, onde o peso de cada avaliador Ã© `wáµ¢ = max(Ïáµ¢, 0)` (Ïáµ¢ = correlaÃ§Ã£o de Spearman do avaliador com o grupo).
4. **CorrelaÃ§Ã£o com mÃ©trica automÃ¡tica**
   * Spearman Ï entre `H_*` e `ragas_*_rating`.
5. **Peso da mÃ©trica automÃ¡tica**
   ```python
   w_A = min(max(Ï, 0), 0.30)  # teto recomendado para dados ruidosos
   ```
6. **Score composto por sumÃ¡rio**
   ```python
   Score = (H_* + w_A * ragas_*_rating) / (1 + w_A)
   ```
7. **ExportaÃ§Ã£o de resultados**
   * `openai_results.csv` & `llama_results.csv` (um Score por linha).
8. **ComparaÃ§Ã£o direta**
   * ConcatenaÃ§Ã£o:
     ```python
     df_scores = pd.concat([
         df_llama['Score'].rename('score_llama'),
         df_openai['Score'].rename('score_openai')
     ], axis=1)
     df_scores['veredito'] = np.where(
         df_scores['score_llama'] >= df_scores['score_openai'], 'llama', 'openai'
     )
     ```
9. **VisualizaÃ§Ãµes** â€” o notebook cria tabelas e grÃ¡ficos simples (CPUâ€‘only).

---

## ğŸ—ƒï¸ Estrutura do RepositÃ³rio

```text
.
â”œâ”€â”€ data/
â”‚   â””â”€â”€ summaries_global_evaluation.csv
â”œâ”€â”€ outputs/                # Gerado apÃ³s executar o notebook
â”‚   â”œâ”€â”€ openai_results.csv
â”‚   â””â”€â”€ llama_results.csv
â”œâ”€â”€ summaries_evaluation.ipynb
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Requisitos

* **PythonÂ â‰¥Â 3.10**
* DependÃªncias principais (ver `requirements.txt`): `pandas`, `numpy`, `pingouin`, `scipy`, `jupyter`.

### Passo a passo para reproduzir
```bash
# 1) Clone o repositÃ³rio
git clone https://github.com/vtrnduda/summarization-metrics-analysis.git
cd summarization-metrics-analysis

# 2) Ambiente virtual (opcional)
python -m venv .venv
source .venv/bin/activate  # Linux/macOS
# .venv\Scripts\activate  # Windows

# 3) Instale as dependÃªncias
pip install -r requirements.txt

# 4) Execute o notebook
jupyter lab  # (ou jupyter notebook)
```
Execute todas as cÃ©lulas. Os CSVs de saÃ­da vÃ£o aparecer em `outputs/`.

---

## ğŸ“‘ SaÃ­das Esperadas

| Arquivo                        | ConteÃºdo                                        |
| ------------------------------ | ---------------------------------------------- |
| `outputs/openai_results.csv`   | Colunas `text`, `Score`, mÃ©tricas intermediÃ¡rias |
| `outputs/llama_results.csv`    | Idem para o LLaMA                               |
| `outputs/df_scores.csv`        | `score_openai`, `score_llama`, `veredito`       |

A Ãºltima cÃ©lula do notebook exibe `df_scores` diretamente em tela.

---

## ğŸ¤ Contribuindo

1. *Fork* â¡ï¸  `git checkout -b minhaâ€‘feature` â¡ï¸  commit â¡ï¸  push â¡ï¸  Pull Request.
2. Mantenha a mesma nomenclatura de colunas para que o notebook funcione sem ajustes.

